## Финальный проект.

## Задание

Вы должны реализовать пайплайн обработки данных из нескольких источников и полноценное хранилище для финтех-стартапа. Выгрузка данных из S3 в DWH с помощью DAG в Airflow: обработка информации в рамках ETL-процесса. Подробное описание данных в файле [data_description.md](/de_project_final/data_description.md).
Хранилище в компании должно быть реализовано на Vertica. После построения пайплайна обработки данных необходимо отдельно реализовать пайплайн формирования витрины с помощью Airflow и DAG. Необходимо также реализовать BI-аналитику для компании: подключиться из Metabase к Vertica и реализовать дашборд.

## Описание решения

В рамках финального проекта были разработаны два dag:
- 1_data_import - загрузка данных в STAGING
- 2_datamart_update - загрузка данных в витрину в DWH

Весь процесс наполнения stg и витрины совмещен в одном классе VerticaLoad, модуль main.py.

### Описание скриптов:

- main.py - модуль, содержащий основной класс VerticaLoad и логику для обработки входных параметров при запуске через bash.
- config.py - модуль, содержащий класс Config, хранящий параметры всех подключений, предусмотрена возможность получения параметров подключения из переменных Ariflow.
- connectors - папка с модулями для подключений к системам: 
    - Postgres - модуль PgConnect, класс PgConnect, подключение к базе - PostgreSQL, в данный момент не используется
    - S3 - модуль s3, класс s3_conn, подключение к хранилищу S3
    - Vertica - модуль VerticaConnect, класс VerticaConnect, подключение к хранилищу Vertica
- loader - папка с модулями для загрузки данных из систем
    - модуль s3 - класс from_s3, загрузка из S3
    - модуль FromPostgres - загрузка из Postgres, в данный момент не используется 
- repository - папка с модулями для сохранения данных в Vertica
    - модуль ToVertica - класс ToVertica - содержит логику для создания таблиц STG, DWH, записи данных в таблицы STG, DWH. DDL-скрипты вынесены в отдельный класс SQLscripts. 


### Логика работы скриптов.

Данные загружаются инкрементально, таблица srv_wf_settings в Vertica содержит информацию об уже загруженных записях. При загрузке из S3 в srv_wf_settings сохраняется список файлов из бакета и даты их обновления. При каждом запуске скрипт получает метаданные файлов из S3 и сравнивает с сохраненными параметрами, загружая только обновленные файлы. При первичном запуске последовательно загружаются все таблицы. Подключение, выгрузка и загрузка данных изолированы в отдельных классах.