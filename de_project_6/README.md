## Описание задачи

Необходимо добавить данные из S3 в разработанное ранее хранилище и выяснить, у каких пабликов самая высокая конверсия. ERD-диаграмма хранилища находится в [корневой папке проекта](/de_project_6/scheme.png). SQL скрипты для его заполнения вынесены в папку [Data Vault](/de_project_6/src/sql/Data%20Vault/).

## Шаги решения
* Загружаем новые данные из S3 хранилища в локальный файл group_log.
* Создаем таблицу для сырых данных в Staging слое хранилища. DDL таблицы находится в файле SQL/STG_DDL.sql.
* Переносим данные из файла в таблицу STG.group_log.
* Создаем таблицы в слое DDS: таблицу связей l_user_group_activity и таблицу-сателлит s_auth_history. Исходный код для создания таблиц находится в файле SQL/DDS_DLL.sql.
* Очищаем данные и загружаем в слой DDS. 
    * В результате предварительного исследования данных выяснилось, что в исходнике дублируются записи: для одного и того же пользователя существует несколько последовательных событий добавления в группу (add) без предварительного выхода из группы (leave). Кроме того, в отдельных случаях идентификатор пригласившего пользователя (user_id_from) совпадал с идентификатором самого пользователя. Перед загрузкой в DDS слой из сырых данных были исключены повторные записи, а в описанном выше случае совпадения идентификаторов пользователя полю user_id_from было присвоено значение null. Исходный код заполнения DDS-таблиц в файлах l_user_group_activity.sql и s_auth_history.sql, соответственно. Перед заполнением из таблиц удаляются старые данные.
* Создаем представление group_conversion в соответствии с заданием: хэш-ключи десяти самых старых групп, количество новых пользователей группы, количество пользователей группы, которые написали хотя бы одно сообщение, доля пользователей группы, которые начали общаться. Были созданы вспомогательные CTE: user_group_log и user_group_messages, исходный код в файлах sql/user_group_log.sql и sql/user_group_messages.sql, соответственно. Код представления в файле sql/users_conversion.sql.

Все этапы процесса были собраны в единый DAG, для запуска использовался оркестратор **Airflow**. Для хранения и обработки данных использовалась СУБД **Vertica**. Исходный код на Python в файле [dags/sprint6-project.py](/de_project_6/src/dags/sprint6-project.py). Использованные библиотеки Python: Airflow, pendulum, boto3, vertica_python, json.